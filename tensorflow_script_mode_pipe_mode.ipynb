{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Script Mode with Pipe Mode Input\n",
    "\n",
    "\n",
    "SageMaker Pipe Mode is an input mechanism for SageMaker training containers based on Linux named pipes. SageMaker makes the data available to the training container using named pipes, which allows data to be downloaded from S3 to the container while training is running. For larger datasets, this dramatically improves the time to start training, as the data does not need to be first downloaded to the container. To learn more about pipe mode, please consult the AWS documentation at: https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-trainingdata.\n",
    "\n",
    "In this tutorial, we show you how to train a TensorFlow estimator using data read with SageMaker Pipe Mode. We use the SageMaker PipeModeDataset class - a special TensorFlow Dataset built specifically to read from SageMaker Pipe Mode data. This Dataset is available in our TensorFlow containers for TensorFlow versions 1.7.0 and up. It's also open-sourced at https://github.com/aws/sagemaker-tensorflow-extensions and can be built into custom TensorFlow images for use in SageMaker.\n",
    "\n",
    "Although you can also build the PipeModeDataset into your own containers, in this tutorial we'll show how you can use the PipeModeDataset by launching training from the SageMaker Python SDK. The SageMaker Python SDK helps you deploy your models for training and hosting in optimized, production-ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow and many other deep learning frameworks.\n",
    "\n",
    "Different collections of S3 files can be made available to the training container while it's running. These are referred to as \"channels\" in SageMaker. In this example, we use two channels - one for training data and one for evaluation data. Each channel is mapped to S3 files from different directories. The SageMaker PipeModeDataset knows how to read from the named pipes for each channel given just the channel name. When we launch SageMaker training we tell SageMaker what channels we have and where in S3 to read the data for each channel.\n",
    "\n",
    "\n",
    "## Setup\n",
    "The following code snippet sets up some variables we'll need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = \"s3://{}/tensorflow_scriptmode_pipemode/customcode\".format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = \"s3://{}/tensorflow_scriptmode_pipemode/artifacts\".format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete training source code\n",
    "\n",
    "In this tutorial we train a TensorFlow LinearClassifier using pipe mode data. The TensorFlow training script is contained in following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcontrib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m map_and_batch\r\n",
      "\r\n",
      "PREFETCH_SIZE = \u001b[34m10\u001b[39;49;00m\r\n",
      "BATCH_SIZE = \u001b[34m64\u001b[39;49;00m\r\n",
      "NUM_PARALLEL_BATCHES = \u001b[34m2\u001b[39;49;00m\r\n",
      "DIMENSION = \u001b[34m1024\u001b[39;49;00m\r\n",
      "EPOCHS = \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_input_fn\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"Returns input function that would feed the model during training\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _input_fn(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32meval_input_fn\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"Returns input function that would feed the model during evaluation\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _input_fn(\u001b[33m\"\u001b[39;49;00m\u001b[33meval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_input_fn\u001b[39;49;00m(channel):\r\n",
      "    \u001b[33m\"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\u001b[39;49;00m\r\n",
      "    features = {\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.FixedLenFeature([], tf.string),\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mparse\u001b[39;49;00m(record):\r\n",
      "        parsed = tf.parse_single_example(record, features)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m ({\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.decode_raw(parsed[\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], tf.float64)}, parsed[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    ds = PipeModeDataset(channel)\r\n",
      "    \u001b[34mif\u001b[39;49;00m EPOCHS > \u001b[34m1\u001b[39;49;00m:\r\n",
      "        ds = ds.repeat(EPOCHS)\r\n",
      "    ds = ds.prefetch(PREFETCH_SIZE)\r\n",
      "    ds = ds.apply(\r\n",
      "        map_and_batch(parse, batch_size=BATCH_SIZE, num_parallel_batches=NUM_PARALLEL_BATCHES)\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m ds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\r\n",
      "    \u001b[37m# model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mserving_input_fn\u001b[39;49;00m():\r\n",
      "    inputs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.placeholder(tf.string)}\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args, unknown = _parse_args()\r\n",
      "\r\n",
      "    column = tf.feature_column.numeric_column(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, shape=(DIMENSION,))\r\n",
      "    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=\u001b[34m3000\u001b[39;49;00m)\r\n",
      "    eval_spec = tf.estimator.EvalSpec(eval_input_fn)\r\n",
      "    linear_classifier = tf.estimator.LinearClassifier(\r\n",
      "        feature_columns=[column], model_dir=args.model_dir\r\n",
      "    )\r\n",
      "    tf.estimator.train_and_evaluate(linear_classifier, train_spec, eval_spec)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\r\n",
      "        linear_classifier.export_savedmodel(args.sm_model_dir, serving_input_fn)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"pipemode.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script is compatible with the SageMaker TensorFlow script mode container. (See: [Preparing TensorFlow Training Script](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#preparing-a-script-mode-training-script)).\n",
    "\n",
    "Using a `PipeModeDataset` to train an estimator using a Pipe Mode channel, we can construct an function that reads from the channel and return an `PipeModeDataset`. This is a TensorFlow Dataset specifically created to read from a SageMaker Pipe Mode channel. A `PipeModeDataset` is a fully-featured TensorFlow Dataset and can be used in exactly the same ways as a regular TensorFlow Dataset can be used.\n",
    "\n",
    "The training and evaluation data used in this tutorial is synthetic. It contains a series of records stored in a TensorFlow Example protobuf object. Each record contains a numeric class label and an array of 1024 floating point numbers. Each array is sampled from a multi-dimensional Gaussian distribution with a class-specific mean. This means it is possible to learn a model using a TensorFlow Linear classifier which can classify examples well. Each record is separated using RecordIO encoding (though the `PipeModeDataset` class also supports the TFRecord format as well).\n",
    "\n",
    "The training and evaluation data were produced using the benchmarking source code in the sagemaker-tensorflow-extensions benchmarking sub-package. If you want to investigate this further, please visit the GitHub repository for sagemaker-tensorflow-extensions at https://github.com/aws/sagemaker-tensorflow-extensions.\n",
    "\n",
    "The following example code shows how to construct a `PipeModeDataset`.\n",
    "\n",
    "```python\n",
    "from sagemaker_tensorflow import `PipeModeDataset`\n",
    "\n",
    "\n",
    "# Simple example data - a labeled vector.\n",
    "features = {\n",
    "    'data': tf.FixedLenFeature([], tf.string),\n",
    "    'labels': tf.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "# A function to parse record bytes to a labeled vector record\n",
    "def parse(record):\n",
    "    parsed = tf.parse_single_example(record, features)\n",
    "    return ({\n",
    "        'data': tf.decode_raw(parsed['data'], tf.float64)\n",
    "    }, parsed['labels'])\n",
    "\n",
    "# Construct a `PipeModeDataset` reading from a 'training' channel, using\n",
    "# the TF Record encoding.\n",
    "ds = `PipeModeDataset`(channel='training', record_format='TFRecord')\n",
    "\n",
    "# The `PipeModeDataset` is a TensorFlow Dataset and provides standard Dataset methods\n",
    "ds = ds.repeat(20)\n",
    "ds = ds.prefetch(10)\n",
    "ds = ds.map(parse, num_parallel_calls=10)\n",
    "ds = ds.batch(64)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running training using the Python SDK\n",
    "\n",
    "We can use the SDK to run our local training script on SageMaker infrastructure.\n",
    "\n",
    "1. Pass the path to the pipemode.py file, which contains the functions for defining your estimator, to the ``sagemaker.tensorflow.TensorFlow`` init method.\n",
    "2. Pass the S3 location that we uploaded our data to previously to the ``fit()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# tensorflow = TensorFlow(\n",
    "#     entry_point=\"pipemode.py\",\n",
    "#     role=role,\n",
    "#     framework_version=\"1.15.3\",\n",
    "#     input_mode=\"Pipe\",\n",
    "#     output_path=model_artifacts_location,\n",
    "#     code_location=custom_code_upload_location,\n",
    "#     train_instance_count=1,\n",
    "#     py_version=\"py3\",\n",
    "#     train_instance_type=\"ml.c4.xlarge\",\n",
    "# #     train_instance_type=\"local\",\n",
    "# )\n",
    "\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "tensorflow = TensorFlow(\n",
    "    entry_point=\"pipemode_2_3.py\",\n",
    "    role=role,\n",
    "    framework_version=\"2.2\",\n",
    "    input_mode=\"Pipe\",\n",
    "    output_path=model_artifacts_location,\n",
    "    code_location=custom_code_upload_location,\n",
    "    train_instance_count=1,\n",
    "    py_version=\"py37\",\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "#     train_instance_type=\"local\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've created the SageMaker Python SDK TensorFlow object, we can call ``fit()`` to launch TensorFlow training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Redirecting to /bin/systemctl stop docker.service\n",
      "Warning: Stopping docker.service, but it can still be activated by:\n",
      "  docker.socket\n",
      "mkdir: cannot create directory ‘/home/ec2-user/SageMaker/docker_disk’: File exists\n",
      "mv: ‘/var/lib/docker’ and ‘/home/ec2-user/SageMaker/docker_disk/docker’ are the same file\n",
      "ln: failed to create symbolic link ‘/var/lib/docker’: File exists\n",
      "Redirecting to /bin/systemctl start docker.service\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "sudo service docker stop\n",
    "mkdir ~/SageMaker/docker_disk\n",
    "sudo mv /var/lib/docker ~/SageMaker/docker_disk/\n",
    "sudo ln -s  ~/SageMaker/docker_disk/docker/ /var/lib/\n",
    "sudo service docker start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/eval/file_000002.recordio to tensorflow-pipe-mode/eval/file_000002.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/eval/file_000000.recordio to tensorflow-pipe-mode/eval/file_000000.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/eval/file_000001.recordio to tensorflow-pipe-mode/eval/file_000001.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/eval/file_000003.recordio to tensorflow-pipe-mode/eval/file_000003.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/eval/file_000004.recordio to tensorflow-pipe-mode/eval/file_000004.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/train/file_000000.recordio to tensorflow-pipe-mode/train/file_000000.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/train/file_000003.recordio to tensorflow-pipe-mode/train/file_000003.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/train/file_000004.recordio to tensorflow-pipe-mode/train/file_000004.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/train/file_000001.recordio to tensorflow-pipe-mode/train/file_000001.recordio\n",
      "download: s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/train/file_000002.recordio to tensorflow-pipe-mode/train/file_000002.recordio\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive  s3://sagemaker-sample-data-us-west-2/tensorflow/pipe-mode/  tensorflow-pipe-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.66.2)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.70.0.tar.gz (466 kB)\n",
      "\u001b[K     |████████████████████████████████| 466 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Collecting boto3>=1.20.18\n",
      "  Downloading boto3-1.20.21-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 47.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.15.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (20.9)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.8)\n",
      "Collecting botocore<1.24.0,>=1.23.21\n",
      "  Downloading botocore-1.23.21-py3-none-any.whl (8.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.4 MB 46.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.21->boto3>=1.20.18->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.21->boto3>=1.20.18->sagemaker) (1.26.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.70.0-py2.py3-none-any.whl size=649149 sha256=04c7305bb36b07afa6e92ce0c4b8d1878016925c62f8a20d6842cdaaa6c86d35\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/da/11/20/c45ef599886a2b1399effa68f80b98b2166dc624e19636c303\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: botocore, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.21.38\n",
      "    Uninstalling botocore-1.21.38:\n",
      "      Successfully uninstalled botocore-1.21.38\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.18.38\n",
      "    Uninstalling boto3-1.18.38:\n",
      "      Successfully uninstalled boto3-1.18.38\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.66.2\n",
      "    Uninstalling sagemaker-2.66.2:\n",
      "      Successfully uninstalled sagemaker-2.66.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.20.24 requires botocore==1.21.24, but you have botocore 1.23.21 which is incompatible.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.23.21 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.20.21 botocore-1.23.21 sagemaker-2.70.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-09 04:31:50 Starting - Starting the training job...\n",
      "2021-12-09 04:32:13 Starting - Launching requested ML instancesProfilerReport-1639024309: InProgress\n",
      "...\n",
      "2021-12-09 04:32:49 Starting - Preparing the instances for training............\n",
      "2021-12-09 04:34:48 Downloading - Downloading input data\n",
      "2021-12-09 04:34:48 Training - Downloading the training image...\n",
      "2021-12-09 04:35:14 Training - Training image download completed. Training in progress.\u001b[34m2021-12-09 04:35:05.905592: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:05.913596: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:06.092766: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10,093 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10,099 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10,607 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10,625 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10,642 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10,654 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2021-12-09-04-31-49-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/customcode/tensorflow-training-2021-12-09-04-31-49-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pipemode_2_3\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pipemode_2_3.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=pipemode_2_3.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=pipemode_2_3\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/customcode/tensorflow-training-2021-12-09-04-31-49-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2021-12-09-04-31-49-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/customcode/tensorflow-training-2021-12-09-04-31-49-916/source/sourcedir.tar.gz\",\"module_name\":\"pipemode_2_3\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pipemode_2_3.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 pipemode_2_3.py --model_dir s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model\u001b[0m\n",
      "\u001b[34mdataset: <TakeDataset shapes: ({data: (None, None)}, (None,)), types: ({data: tf.float64}, tf.int64)>\u001b[0m\n",
      "\u001b[34mEPOCHS: 1\u001b[0m\n",
      "\u001b[34m[2021-12-09 04:35:14.078 ip-10-0-78-215.us-west-2.compute.internal:23 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-12-09 04:35:14.078 ip-10-0-78-215.us-west-2.compute.internal:23 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-12-09 04:35:14.079 ip-10-0-78-215.us-west-2.compute.internal:23 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-12-09 04:35:14.079 ip-10-0-78-215.us-west-2.compute.internal:23 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-12-09 04:35:14.289 ip-10-0-78-215.us-west-2.compute.internal:23 INFO hook.py:398] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34mdataset: <TakeDataset shapes: ({data: (None, None)}, (None,)), types: ({data: tf.float64}, tf.int64)>\u001b[0m\n",
      "\u001b[34mEPOCHS: 1\u001b[0m\n",
      "\u001b[34m[2021-12-09 04:35:22.128 ip-10-0-78-215.us-west-2.compute.internal:23 INFO hook.py:398] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2021-12-09 04:35:10.975689: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10.975825: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:106] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:10.996121: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:425] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using default config.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\u001b[0m\n",
      "\u001b[34mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Not using Distribute Coordinator.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running training and evaluation locally (non-distributed).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From pipemode_2_3.py:48: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:27,498 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\u001b[0m\n",
      "\u001b[34mIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\u001b[0m\n",
      "\u001b[34mTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:540: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `layer.add_weight` method instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model/model.ckpt-407\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse standard file utilities to get mtimes.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 407...\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 407 into s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 407...\u001b[0m\n",
      "\u001b[34m2021-12-09 04:35:16.575004: W tensorflow/core/framework/dataset.cc:448] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.015776774, step = 407\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 141.216\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.012249694, step = 507 (0.709 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 222.849\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.00968622, step = 607 (0.449 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 272.902\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.009713685, step = 707 (0.366 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:global_step/sec: 318.557\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:loss = 0.0083977925, step = 807 (0.314 sec)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 814...\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving checkpoints for 814 into s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model/model.ckpt.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 814...\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\u001b[0m\n",
      "\u001b[34mIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\u001b[0m\n",
      "\u001b[34mTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Starting evaluation at 2021-12-09T04:35:22Z\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model/model.ckpt-814\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [10/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [20/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [30/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [40/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [50/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [60/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [70/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [80/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [90/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Evaluation [100/100]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Inference Time : 2.96376s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished evaluation at 2021-12-09-04:35:25\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving dict for global step 814: accuracy = 1.0, accuracy_baseline = 0.5, auc = 1.0, auc_precision_recall = 1.0, average_loss = 0.008086874, global_step = 814, label/mean = 0.5, loss = 0.008086874, precision = 1.0, prediction/mean = 0.49222103, recall = 1.0\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Saving 'checkpoint_path' summary for global step 814: s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model/model.ckpt-814\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loss for final step: 0.008507295.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:145: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-230755935769/tensorflow_scriptmode_pipemode/artifacts/tensorflow-training-2021-12-09-04-12-53-857/model/model.ckpt-814\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-1639024525/saved_model.pb\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-12-09 04:35:38 Uploading - Uploading generated training model\n",
      "2021-12-09 04:35:38 Completed - Training job completed\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n",
      "CPU times: user 717 ms, sys: 14.5 ms, total: 732 ms\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "\n",
    "# use the region-specific sample data bucket\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "train_data = \"s3://sagemaker-us-west-2-230755935769/experiment/\"\n",
    "eval_data = \"s3://sagemaker-us-west-2-230755935769/experiment/\"\n",
    "# s3://sagemaker-us-west-2-230755935769/experiment\n",
    "\n",
    "# train_data = \"s3://sagemaker-sample-data-{}/tensorflow/pipe-mode/train\".format(region)\n",
    "# eval_data = \"s3://sagemaker-sample-data-{}/tensorflow/pipe-mode/eval\".format(region)\n",
    "\n",
    "# train_data = \"s3://sagemaker-sample-data-{}/tensorflow/pipe-mode/\".format(region)\n",
    "# eval_data = \"s3://sagemaker-sample-data-{}/tensorflow/pipe-mode/\".format(region)\n",
    "\n",
    "tensorflow.fit({\"train\": train_data, \"eval\": eval_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training finishes, the trained model artifacts will be uploaded to S3. This following example notebook shows how to deploy a model trained with script mode: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
